{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Import libraries\r\n",
        "from pyspark.sql.functions import col, pandas_udf,udf,lit\r\n",
        "from azureml.core import Workspace\r\n",
        "from azureml.core.authentication import ServicePrincipalAuthentication\r\n",
        "import azure.synapse.ml.predict as pcontext\r\n",
        "import azure.synapse.ml.predict.utils._logger as synapse_predict_logger"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set input data path\r\n",
        "DATA_FILE = \"abfss://<containername>@<storageaccountname>.dfs.core.windows.net/dailyjoininput.csv\"\r\n",
        "#Set model URI\r\n",
        "#Set AML URI, if trained model is registered in AML\r\n",
        "AML_MODEL_URI = \"aml://churns_mlflow:1\" #In URI \":x\" signifies model version in AML. You can   choose which model version you want to run. If \":x\" is not provided then by default   latest version will be picked.\r\n",
        "\r\n",
        "#Define model return type\r\n",
        "RETURN_TYPES = \"bigint\" # for ex: int, float etc. PySpark data types are supported\r\n",
        "\r\n",
        "#Define model runtime. This supports only mlflow\r\n",
        "RUNTIME = \"mlflow\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#AML workspace authentication using linked service\r\n",
        "from notebookutils.mssparkutils import azureML\r\n",
        "ws = azureML.getWorkspace(\"<linked_service_name>\") #   \"<linked_service_name>\" is the linked service name, not AML workspace name. Also, linked   service supports MSI and service principal both"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Enable SynapseML predict\r\n",
        "spark.conf.set(\"spark.synapse.ml.predict.enabled\",\"true\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Bind model within Spark session\r\n",
        "model = pcontext.bind_model(\r\n",
        "    return_types=RETURN_TYPES, \r\n",
        "    runtime=RUNTIME, \r\n",
        "    model_alias=\"<your model alias>\", #This alias will be used in PREDICT call to refer  this   model\r\n",
        "    model_uri=AML_MODEL_URI, #In case of AML, it will be AML_MODEL_URI\r\n",
        "    aml_workspace=ws #This is only for AML. In case of ADLS, this parameter can be removed\r\n",
        "    ).register()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, LongType, StringType\r\n",
        "\r\n",
        "churn_schm = StructType([\r\n",
        "        StructField(\"customer_id\", StringType(), True),\r\n",
        "        StructField(\"current_linkedin_activity\", LongType(), True),\r\n",
        "        StructField(\"email_domain\", StringType(), True),\r\n",
        "        StructField(\"linkedin_skill_code\", LongType(), True),\r\n",
        "        StructField(\"mentor_program_involvement\", LongType(), True),\r\n",
        "        StructField(\"negative_review_in_past_5_years\", LongType(), True),\r\n",
        "        StructField(\"recruiting_location_code\", LongType(),True),\r\n",
        "        StructField(\"recruiting_method_code\", LongType(), True),\r\n",
        "        StructField(\"weekly_consumption\", LongType(), True),\r\n",
        "        StructField(\"years_of_membership\", LongType(), True),\r\n",
        "        StructField(\"survey_attitude_towards_company\", LongType(), True),\r\n",
        "        StructField(\"survey_attitude_towards_product_features\", LongType(), True),\r\n",
        "        StructField(\"survey_attitude_towards_performance\", LongType(), True),\r\n",
        "        StructField(\"survey_attitude_towards_usability\", LongType(), True),\r\n",
        "        StructField(\"survey_attitude_towards_product_quality\", LongType(), True),\r\n",
        "        StructField(\"survey_attitude_towards_customer_service\", LongType(), True)\r\n",
        "    ])\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load test data\r\n",
        "df = spark.read \\\r\n",
        "    .format(\"csv\") \\\r\n",
        "    .option(\"header\", \"true\") \\\r\n",
        "    .schema(churn_schm) \\\r\n",
        "    .csv(DATA_FILE) \r\n",
        "df = df.select(df.columns[1:])\r\n",
        "df.createOrReplaceTempView('data')\r\n",
        "df.show(10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "\r\n",
        "select * from data"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call PREDICT\r\n",
        "\r\n",
        "predictions = spark.sql(\r\n",
        "                    \"\"\"\r\n",
        "                        SELECT PREDICT('<your model alias>', *) AS predict FROM data\r\n",
        "                    \"\"\"\r\n",
        "                 ).show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}